{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep Residual Network.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfPhsUu7Z3J9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8mTt-DS4cyzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Device configuration\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtI-i4lQc9AS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Hyper-parameters\n",
        "\n",
        "num_epochs = 80\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-9SRiyFdAzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Image preprocessing modules\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomCrop(32),\n",
        "    transforms.ToTensor()\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzs0ISxzdvtW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "2b166227-1ffb-43b4-edd2-f4a2cd1f0cf7"
      },
      "source": [
        "## CIFAR-10 dataset\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root = '../../data/',\n",
        "                                             train = True,\n",
        "                                             transform = transform,\n",
        "                                             download = True)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root = '../../data/',\n",
        "                                            train = False,\n",
        "                                            transform = transforms.ToTensor())"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ../../data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "170500096it [00:06, 27639155.47it/s]                               \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bWPl0UzdeKm8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Data loader\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset = train_dataset,\n",
        "                                           batch_size = 100,\n",
        "                                           shuffle = True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset = test_dataset,\n",
        "                                          batch_size = 100,\n",
        "                                          shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2G3Lh_DehYn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 3 x 3 convolution\n",
        "\n",
        "def conv3x3(in_channels, out_channels, stride = 1):\n",
        "  return nn.Conv2d(in_channels, out_channels, \n",
        "                   kernel_size = 3, stride = stride,\n",
        "                   padding = 1, bias = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnwzJjqVfNBo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Residual block\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "  \n",
        "  def __init__(self, in_channels, out_channels, stride = 1, downsample = None):\n",
        "    super(ResidualBlock, self).__init__()\n",
        "    self.conv1 = conv3x3(in_channels, out_channels, stride)\n",
        "    self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.conv2 = conv3x3(out_channels, out_channels)\n",
        "    self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "    self.downsample = downsample\n",
        "    \n",
        "  def forward(self, x):\n",
        "    residual = x\n",
        "    out = self.conv1(x)\n",
        "    out = self.bn1(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.conv2(out)\n",
        "    out = self.bn2(out)\n",
        "    if self.downsample:\n",
        "      residual = self.downsample(x)\n",
        "    out += residual\n",
        "    out = self.relu(out)\n",
        "    return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QU6xzh5bhhK8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## ResNet\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "  \n",
        "  def __init__(self, block, layers, num_classes = 10):\n",
        "    super(ResNet, self).__init__()\n",
        "    self.in_channels = 16\n",
        "    self.conv = conv3x3(3, 16)\n",
        "    self.bn = nn.BatchNorm2d(16)\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "    self.layer1 = self.make_layer(block, 16, layers[0])\n",
        "    self.layer2 = self.make_layer(block, 32, layers[1], 2)\n",
        "    self.layer3 = self.make_layer(block, 64, layers[2], 2)\n",
        "    self.avg_pool = nn.AvgPool2d(8)\n",
        "    self.fc = nn.Linear(64, num_classes)\n",
        "    \n",
        "  def make_layer(self, block, out_channels, blocks, stride = 1):\n",
        "    downsample = None\n",
        "    if (stride != 1) or (self.in_channels != out_channels):\n",
        "      downsample = nn.Sequential(\n",
        "          conv3x3(self.in_channels, out_channels, stride = stride),\n",
        "          nn.BatchNorm2d(out_channels)\n",
        "      )\n",
        "    layers = []\n",
        "    layers.append(block(self.in_channels, out_channels, stride, downsample))\n",
        "    self.in_channels = out_channels\n",
        "    for i in range(1, blocks):\n",
        "      layers.append(block(out_channels, out_channels))\n",
        "    return nn.Sequential(*layers)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    out = self.conv(x)\n",
        "    out = self.bn(out)\n",
        "    out = self.relu(out)\n",
        "    out = self.layer1(out)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    out = self.avg_pool(out)\n",
        "    out = out.view(out.size(0), -1)\n",
        "    out = self.fc(out)\n",
        "    return out\n",
        "\n",
        "model = ResNet(ResidualBlock, [2, 2, 2]).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep7VGXskUlGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Loss and optimizer\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(),\n",
        "                             lr = learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lc5FyMudWkD-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## For updating learning rate\n",
        "\n",
        "def update_lr(optimizer, lr):\n",
        "  for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = lr"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z26iG_AMXB7I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "820af124-b9e6-4cfe-8726-6952263a789e"
      },
      "source": [
        "## Train the model\n",
        "\n",
        "total_step = len(train_loader)\n",
        "curr_lr = learning_rate\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (images, labels) in enumerate(train_loader):\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    \n",
        "    ## Forward pass\n",
        "    outputs = model(images)\n",
        "    loss = criterion(outputs, labels)\n",
        "    \n",
        "    ## Backward pass and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    if (i + 1) % 100 == 0:\n",
        "      print (\"Epoch [{} / {}], Step [{} / {}], Loss: {:.4f}\"\n",
        "             .format(epoch + 1, num_epochs, i + 1, total_step, loss.item()))\n",
        "      \n",
        "  ## Decay learning rate\n",
        "  if (epoch + 1) % 20 == 0:\n",
        "    curr_lr /= 3\n",
        "    update_lr(optimizer, curr_lr)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [1 / 80], Step [100 / 500], Loss: 1.8448\n",
            "Epoch [1 / 80], Step [200 / 500], Loss: 1.5901\n",
            "Epoch [1 / 80], Step [300 / 500], Loss: 1.4807\n",
            "Epoch [1 / 80], Step [400 / 500], Loss: 1.1993\n",
            "Epoch [1 / 80], Step [500 / 500], Loss: 1.0643\n",
            "Epoch [2 / 80], Step [100 / 500], Loss: 1.0892\n",
            "Epoch [2 / 80], Step [200 / 500], Loss: 1.0285\n",
            "Epoch [2 / 80], Step [300 / 500], Loss: 1.0698\n",
            "Epoch [2 / 80], Step [400 / 500], Loss: 1.0270\n",
            "Epoch [2 / 80], Step [500 / 500], Loss: 1.0000\n",
            "Epoch [3 / 80], Step [100 / 500], Loss: 0.9735\n",
            "Epoch [3 / 80], Step [200 / 500], Loss: 0.8523\n",
            "Epoch [3 / 80], Step [300 / 500], Loss: 0.9218\n",
            "Epoch [3 / 80], Step [400 / 500], Loss: 0.9838\n",
            "Epoch [3 / 80], Step [500 / 500], Loss: 0.8769\n",
            "Epoch [4 / 80], Step [100 / 500], Loss: 0.6579\n",
            "Epoch [4 / 80], Step [200 / 500], Loss: 0.8939\n",
            "Epoch [4 / 80], Step [300 / 500], Loss: 0.7660\n",
            "Epoch [4 / 80], Step [400 / 500], Loss: 0.9269\n",
            "Epoch [4 / 80], Step [500 / 500], Loss: 0.6995\n",
            "Epoch [5 / 80], Step [100 / 500], Loss: 0.8089\n",
            "Epoch [5 / 80], Step [200 / 500], Loss: 0.7573\n",
            "Epoch [5 / 80], Step [300 / 500], Loss: 0.8119\n",
            "Epoch [5 / 80], Step [400 / 500], Loss: 0.6743\n",
            "Epoch [5 / 80], Step [500 / 500], Loss: 0.7316\n",
            "Epoch [6 / 80], Step [100 / 500], Loss: 0.7047\n",
            "Epoch [6 / 80], Step [200 / 500], Loss: 0.7469\n",
            "Epoch [6 / 80], Step [300 / 500], Loss: 0.6196\n",
            "Epoch [6 / 80], Step [400 / 500], Loss: 0.6650\n",
            "Epoch [6 / 80], Step [500 / 500], Loss: 0.7616\n",
            "Epoch [7 / 80], Step [100 / 500], Loss: 0.6480\n",
            "Epoch [7 / 80], Step [200 / 500], Loss: 0.5924\n",
            "Epoch [7 / 80], Step [300 / 500], Loss: 0.6097\n",
            "Epoch [7 / 80], Step [400 / 500], Loss: 0.7440\n",
            "Epoch [7 / 80], Step [500 / 500], Loss: 0.6257\n",
            "Epoch [8 / 80], Step [100 / 500], Loss: 0.5577\n",
            "Epoch [8 / 80], Step [200 / 500], Loss: 0.6139\n",
            "Epoch [8 / 80], Step [300 / 500], Loss: 0.5892\n",
            "Epoch [8 / 80], Step [400 / 500], Loss: 0.4730\n",
            "Epoch [8 / 80], Step [500 / 500], Loss: 0.5713\n",
            "Epoch [9 / 80], Step [100 / 500], Loss: 0.6036\n",
            "Epoch [9 / 80], Step [200 / 500], Loss: 0.5688\n",
            "Epoch [9 / 80], Step [300 / 500], Loss: 0.4961\n",
            "Epoch [9 / 80], Step [400 / 500], Loss: 0.6114\n",
            "Epoch [9 / 80], Step [500 / 500], Loss: 0.5075\n",
            "Epoch [10 / 80], Step [100 / 500], Loss: 0.5479\n",
            "Epoch [10 / 80], Step [200 / 500], Loss: 0.5725\n",
            "Epoch [10 / 80], Step [300 / 500], Loss: 0.5156\n",
            "Epoch [10 / 80], Step [400 / 500], Loss: 0.5967\n",
            "Epoch [10 / 80], Step [500 / 500], Loss: 0.3913\n",
            "Epoch [11 / 80], Step [100 / 500], Loss: 0.6752\n",
            "Epoch [11 / 80], Step [200 / 500], Loss: 0.4985\n",
            "Epoch [11 / 80], Step [300 / 500], Loss: 0.5885\n",
            "Epoch [11 / 80], Step [400 / 500], Loss: 0.3778\n",
            "Epoch [11 / 80], Step [500 / 500], Loss: 0.6269\n",
            "Epoch [12 / 80], Step [100 / 500], Loss: 0.4535\n",
            "Epoch [12 / 80], Step [200 / 500], Loss: 0.5446\n",
            "Epoch [12 / 80], Step [300 / 500], Loss: 0.5096\n",
            "Epoch [12 / 80], Step [400 / 500], Loss: 0.5160\n",
            "Epoch [12 / 80], Step [500 / 500], Loss: 0.5347\n",
            "Epoch [13 / 80], Step [100 / 500], Loss: 0.3857\n",
            "Epoch [13 / 80], Step [200 / 500], Loss: 0.5878\n",
            "Epoch [13 / 80], Step [300 / 500], Loss: 0.5436\n",
            "Epoch [13 / 80], Step [400 / 500], Loss: 0.4191\n",
            "Epoch [13 / 80], Step [500 / 500], Loss: 0.4029\n",
            "Epoch [14 / 80], Step [100 / 500], Loss: 0.4969\n",
            "Epoch [14 / 80], Step [200 / 500], Loss: 0.5021\n",
            "Epoch [14 / 80], Step [300 / 500], Loss: 0.4679\n",
            "Epoch [14 / 80], Step [400 / 500], Loss: 0.5304\n",
            "Epoch [14 / 80], Step [500 / 500], Loss: 0.3587\n",
            "Epoch [15 / 80], Step [100 / 500], Loss: 0.3689\n",
            "Epoch [15 / 80], Step [200 / 500], Loss: 0.5601\n",
            "Epoch [15 / 80], Step [300 / 500], Loss: 0.3493\n",
            "Epoch [15 / 80], Step [400 / 500], Loss: 0.3081\n",
            "Epoch [15 / 80], Step [500 / 500], Loss: 0.4007\n",
            "Epoch [16 / 80], Step [100 / 500], Loss: 0.4519\n",
            "Epoch [16 / 80], Step [200 / 500], Loss: 0.3807\n",
            "Epoch [16 / 80], Step [300 / 500], Loss: 0.6339\n",
            "Epoch [16 / 80], Step [400 / 500], Loss: 0.4384\n",
            "Epoch [16 / 80], Step [500 / 500], Loss: 0.3888\n",
            "Epoch [17 / 80], Step [100 / 500], Loss: 0.3914\n",
            "Epoch [17 / 80], Step [200 / 500], Loss: 0.4872\n",
            "Epoch [17 / 80], Step [300 / 500], Loss: 0.3597\n",
            "Epoch [17 / 80], Step [400 / 500], Loss: 0.3256\n",
            "Epoch [17 / 80], Step [500 / 500], Loss: 0.3196\n",
            "Epoch [18 / 80], Step [100 / 500], Loss: 0.3658\n",
            "Epoch [18 / 80], Step [200 / 500], Loss: 0.2801\n",
            "Epoch [18 / 80], Step [300 / 500], Loss: 0.5562\n",
            "Epoch [18 / 80], Step [400 / 500], Loss: 0.4642\n",
            "Epoch [18 / 80], Step [500 / 500], Loss: 0.5000\n",
            "Epoch [19 / 80], Step [100 / 500], Loss: 0.5018\n",
            "Epoch [19 / 80], Step [200 / 500], Loss: 0.4220\n",
            "Epoch [19 / 80], Step [300 / 500], Loss: 0.4119\n",
            "Epoch [19 / 80], Step [400 / 500], Loss: 0.2775\n",
            "Epoch [19 / 80], Step [500 / 500], Loss: 0.4244\n",
            "Epoch [20 / 80], Step [100 / 500], Loss: 0.2542\n",
            "Epoch [20 / 80], Step [200 / 500], Loss: 0.3311\n",
            "Epoch [20 / 80], Step [300 / 500], Loss: 0.3527\n",
            "Epoch [20 / 80], Step [400 / 500], Loss: 0.3715\n",
            "Epoch [20 / 80], Step [500 / 500], Loss: 0.1981\n",
            "Epoch [21 / 80], Step [100 / 500], Loss: 0.2251\n",
            "Epoch [21 / 80], Step [200 / 500], Loss: 0.2493\n",
            "Epoch [21 / 80], Step [300 / 500], Loss: 0.2417\n",
            "Epoch [21 / 80], Step [400 / 500], Loss: 0.2531\n",
            "Epoch [21 / 80], Step [500 / 500], Loss: 0.2890\n",
            "Epoch [22 / 80], Step [100 / 500], Loss: 0.2496\n",
            "Epoch [22 / 80], Step [200 / 500], Loss: 0.2354\n",
            "Epoch [22 / 80], Step [300 / 500], Loss: 0.2359\n",
            "Epoch [22 / 80], Step [400 / 500], Loss: 0.4049\n",
            "Epoch [22 / 80], Step [500 / 500], Loss: 0.2802\n",
            "Epoch [23 / 80], Step [100 / 500], Loss: 0.2993\n",
            "Epoch [23 / 80], Step [200 / 500], Loss: 0.3541\n",
            "Epoch [23 / 80], Step [300 / 500], Loss: 0.2919\n",
            "Epoch [23 / 80], Step [400 / 500], Loss: 0.1996\n",
            "Epoch [23 / 80], Step [500 / 500], Loss: 0.3160\n",
            "Epoch [24 / 80], Step [100 / 500], Loss: 0.2430\n",
            "Epoch [24 / 80], Step [200 / 500], Loss: 0.2955\n",
            "Epoch [24 / 80], Step [300 / 500], Loss: 0.2138\n",
            "Epoch [24 / 80], Step [400 / 500], Loss: 0.2174\n",
            "Epoch [24 / 80], Step [500 / 500], Loss: 0.3087\n",
            "Epoch [25 / 80], Step [100 / 500], Loss: 0.3010\n",
            "Epoch [25 / 80], Step [200 / 500], Loss: 0.2182\n",
            "Epoch [25 / 80], Step [300 / 500], Loss: 0.2868\n",
            "Epoch [25 / 80], Step [400 / 500], Loss: 0.2502\n",
            "Epoch [25 / 80], Step [500 / 500], Loss: 0.2725\n",
            "Epoch [26 / 80], Step [100 / 500], Loss: 0.2282\n",
            "Epoch [26 / 80], Step [200 / 500], Loss: 0.2949\n",
            "Epoch [26 / 80], Step [300 / 500], Loss: 0.3766\n",
            "Epoch [26 / 80], Step [400 / 500], Loss: 0.3343\n",
            "Epoch [26 / 80], Step [500 / 500], Loss: 0.1870\n",
            "Epoch [27 / 80], Step [100 / 500], Loss: 0.3380\n",
            "Epoch [27 / 80], Step [200 / 500], Loss: 0.2818\n",
            "Epoch [27 / 80], Step [300 / 500], Loss: 0.3055\n",
            "Epoch [27 / 80], Step [400 / 500], Loss: 0.3039\n",
            "Epoch [27 / 80], Step [500 / 500], Loss: 0.2294\n",
            "Epoch [28 / 80], Step [100 / 500], Loss: 0.1692\n",
            "Epoch [28 / 80], Step [200 / 500], Loss: 0.2579\n",
            "Epoch [28 / 80], Step [300 / 500], Loss: 0.1515\n",
            "Epoch [28 / 80], Step [400 / 500], Loss: 0.2403\n",
            "Epoch [28 / 80], Step [500 / 500], Loss: 0.1972\n",
            "Epoch [29 / 80], Step [100 / 500], Loss: 0.2151\n",
            "Epoch [29 / 80], Step [200 / 500], Loss: 0.2793\n",
            "Epoch [29 / 80], Step [300 / 500], Loss: 0.1809\n",
            "Epoch [29 / 80], Step [400 / 500], Loss: 0.3592\n",
            "Epoch [29 / 80], Step [500 / 500], Loss: 0.1911\n",
            "Epoch [30 / 80], Step [100 / 500], Loss: 0.1629\n",
            "Epoch [30 / 80], Step [200 / 500], Loss: 0.2127\n",
            "Epoch [30 / 80], Step [300 / 500], Loss: 0.2091\n",
            "Epoch [30 / 80], Step [400 / 500], Loss: 0.2523\n",
            "Epoch [30 / 80], Step [500 / 500], Loss: 0.3120\n",
            "Epoch [31 / 80], Step [100 / 500], Loss: 0.3115\n",
            "Epoch [31 / 80], Step [200 / 500], Loss: 0.2458\n",
            "Epoch [31 / 80], Step [300 / 500], Loss: 0.1758\n",
            "Epoch [31 / 80], Step [400 / 500], Loss: 0.3392\n",
            "Epoch [31 / 80], Step [500 / 500], Loss: 0.2563\n",
            "Epoch [32 / 80], Step [100 / 500], Loss: 0.1363\n",
            "Epoch [32 / 80], Step [200 / 500], Loss: 0.2399\n",
            "Epoch [32 / 80], Step [300 / 500], Loss: 0.2886\n",
            "Epoch [32 / 80], Step [400 / 500], Loss: 0.2244\n",
            "Epoch [32 / 80], Step [500 / 500], Loss: 0.3356\n",
            "Epoch [33 / 80], Step [100 / 500], Loss: 0.2614\n",
            "Epoch [33 / 80], Step [200 / 500], Loss: 0.1055\n",
            "Epoch [33 / 80], Step [300 / 500], Loss: 0.2246\n",
            "Epoch [33 / 80], Step [400 / 500], Loss: 0.2328\n",
            "Epoch [33 / 80], Step [500 / 500], Loss: 0.1780\n",
            "Epoch [34 / 80], Step [100 / 500], Loss: 0.2511\n",
            "Epoch [34 / 80], Step [200 / 500], Loss: 0.1483\n",
            "Epoch [34 / 80], Step [300 / 500], Loss: 0.2717\n",
            "Epoch [34 / 80], Step [400 / 500], Loss: 0.3008\n",
            "Epoch [34 / 80], Step [500 / 500], Loss: 0.2228\n",
            "Epoch [35 / 80], Step [100 / 500], Loss: 0.3690\n",
            "Epoch [35 / 80], Step [200 / 500], Loss: 0.1858\n",
            "Epoch [35 / 80], Step [300 / 500], Loss: 0.3247\n",
            "Epoch [35 / 80], Step [400 / 500], Loss: 0.2250\n",
            "Epoch [35 / 80], Step [500 / 500], Loss: 0.1532\n",
            "Epoch [36 / 80], Step [100 / 500], Loss: 0.2036\n",
            "Epoch [36 / 80], Step [200 / 500], Loss: 0.2240\n",
            "Epoch [36 / 80], Step [300 / 500], Loss: 0.1968\n",
            "Epoch [36 / 80], Step [400 / 500], Loss: 0.1686\n",
            "Epoch [36 / 80], Step [500 / 500], Loss: 0.2601\n",
            "Epoch [37 / 80], Step [100 / 500], Loss: 0.1900\n",
            "Epoch [37 / 80], Step [200 / 500], Loss: 0.2955\n",
            "Epoch [37 / 80], Step [300 / 500], Loss: 0.1865\n",
            "Epoch [37 / 80], Step [400 / 500], Loss: 0.1855\n",
            "Epoch [37 / 80], Step [500 / 500], Loss: 0.2327\n",
            "Epoch [38 / 80], Step [100 / 500], Loss: 0.1289\n",
            "Epoch [38 / 80], Step [200 / 500], Loss: 0.2943\n",
            "Epoch [38 / 80], Step [300 / 500], Loss: 0.2165\n",
            "Epoch [38 / 80], Step [400 / 500], Loss: 0.2775\n",
            "Epoch [38 / 80], Step [500 / 500], Loss: 0.1977\n",
            "Epoch [39 / 80], Step [100 / 500], Loss: 0.2038\n",
            "Epoch [39 / 80], Step [200 / 500], Loss: 0.2150\n",
            "Epoch [39 / 80], Step [300 / 500], Loss: 0.2677\n",
            "Epoch [39 / 80], Step [400 / 500], Loss: 0.1194\n",
            "Epoch [39 / 80], Step [500 / 500], Loss: 0.1874\n",
            "Epoch [40 / 80], Step [100 / 500], Loss: 0.1882\n",
            "Epoch [40 / 80], Step [200 / 500], Loss: 0.2525\n",
            "Epoch [40 / 80], Step [300 / 500], Loss: 0.1510\n",
            "Epoch [40 / 80], Step [400 / 500], Loss: 0.2447\n",
            "Epoch [40 / 80], Step [500 / 500], Loss: 0.3128\n",
            "Epoch [41 / 80], Step [100 / 500], Loss: 0.1860\n",
            "Epoch [41 / 80], Step [200 / 500], Loss: 0.2573\n",
            "Epoch [41 / 80], Step [300 / 500], Loss: 0.2758\n",
            "Epoch [41 / 80], Step [400 / 500], Loss: 0.1433\n",
            "Epoch [41 / 80], Step [500 / 500], Loss: 0.1988\n",
            "Epoch [42 / 80], Step [100 / 500], Loss: 0.1673\n",
            "Epoch [42 / 80], Step [200 / 500], Loss: 0.1485\n",
            "Epoch [42 / 80], Step [300 / 500], Loss: 0.1581\n",
            "Epoch [42 / 80], Step [400 / 500], Loss: 0.2353\n",
            "Epoch [42 / 80], Step [500 / 500], Loss: 0.2866\n",
            "Epoch [43 / 80], Step [100 / 500], Loss: 0.1832\n",
            "Epoch [43 / 80], Step [200 / 500], Loss: 0.1641\n",
            "Epoch [43 / 80], Step [300 / 500], Loss: 0.3792\n",
            "Epoch [43 / 80], Step [400 / 500], Loss: 0.2147\n",
            "Epoch [43 / 80], Step [500 / 500], Loss: 0.2350\n",
            "Epoch [44 / 80], Step [100 / 500], Loss: 0.1373\n",
            "Epoch [44 / 80], Step [200 / 500], Loss: 0.1118\n",
            "Epoch [44 / 80], Step [300 / 500], Loss: 0.1567\n",
            "Epoch [44 / 80], Step [400 / 500], Loss: 0.1076\n",
            "Epoch [44 / 80], Step [500 / 500], Loss: 0.1129\n",
            "Epoch [45 / 80], Step [100 / 500], Loss: 0.1614\n",
            "Epoch [45 / 80], Step [200 / 500], Loss: 0.0998\n",
            "Epoch [45 / 80], Step [300 / 500], Loss: 0.1658\n",
            "Epoch [45 / 80], Step [400 / 500], Loss: 0.0787\n",
            "Epoch [45 / 80], Step [500 / 500], Loss: 0.1727\n",
            "Epoch [46 / 80], Step [100 / 500], Loss: 0.1555\n",
            "Epoch [46 / 80], Step [200 / 500], Loss: 0.1862\n",
            "Epoch [46 / 80], Step [300 / 500], Loss: 0.1690\n",
            "Epoch [46 / 80], Step [400 / 500], Loss: 0.2889\n",
            "Epoch [46 / 80], Step [500 / 500], Loss: 0.0799\n",
            "Epoch [47 / 80], Step [100 / 500], Loss: 0.2123\n",
            "Epoch [47 / 80], Step [200 / 500], Loss: 0.1399\n",
            "Epoch [47 / 80], Step [300 / 500], Loss: 0.1741\n",
            "Epoch [47 / 80], Step [400 / 500], Loss: 0.1206\n",
            "Epoch [47 / 80], Step [500 / 500], Loss: 0.1423\n",
            "Epoch [48 / 80], Step [100 / 500], Loss: 0.1088\n",
            "Epoch [48 / 80], Step [200 / 500], Loss: 0.1611\n",
            "Epoch [48 / 80], Step [300 / 500], Loss: 0.1484\n",
            "Epoch [48 / 80], Step [400 / 500], Loss: 0.1461\n",
            "Epoch [48 / 80], Step [500 / 500], Loss: 0.1908\n",
            "Epoch [49 / 80], Step [100 / 500], Loss: 0.1101\n",
            "Epoch [49 / 80], Step [200 / 500], Loss: 0.2349\n",
            "Epoch [49 / 80], Step [300 / 500], Loss: 0.2166\n",
            "Epoch [49 / 80], Step [400 / 500], Loss: 0.1372\n",
            "Epoch [49 / 80], Step [500 / 500], Loss: 0.1188\n",
            "Epoch [50 / 80], Step [100 / 500], Loss: 0.2253\n",
            "Epoch [50 / 80], Step [200 / 500], Loss: 0.1112\n",
            "Epoch [50 / 80], Step [300 / 500], Loss: 0.1461\n",
            "Epoch [50 / 80], Step [400 / 500], Loss: 0.1590\n",
            "Epoch [50 / 80], Step [500 / 500], Loss: 0.2603\n",
            "Epoch [51 / 80], Step [100 / 500], Loss: 0.1095\n",
            "Epoch [51 / 80], Step [200 / 500], Loss: 0.1618\n",
            "Epoch [51 / 80], Step [300 / 500], Loss: 0.2195\n",
            "Epoch [51 / 80], Step [400 / 500], Loss: 0.1718\n",
            "Epoch [51 / 80], Step [500 / 500], Loss: 0.3105\n",
            "Epoch [52 / 80], Step [100 / 500], Loss: 0.1400\n",
            "Epoch [52 / 80], Step [200 / 500], Loss: 0.1883\n",
            "Epoch [52 / 80], Step [300 / 500], Loss: 0.1266\n",
            "Epoch [52 / 80], Step [400 / 500], Loss: 0.2874\n",
            "Epoch [52 / 80], Step [500 / 500], Loss: 0.1260\n",
            "Epoch [53 / 80], Step [100 / 500], Loss: 0.1609\n",
            "Epoch [53 / 80], Step [200 / 500], Loss: 0.1662\n",
            "Epoch [53 / 80], Step [300 / 500], Loss: 0.3510\n",
            "Epoch [53 / 80], Step [400 / 500], Loss: 0.2119\n",
            "Epoch [53 / 80], Step [500 / 500], Loss: 0.2664\n",
            "Epoch [54 / 80], Step [100 / 500], Loss: 0.2258\n",
            "Epoch [54 / 80], Step [200 / 500], Loss: 0.1176\n",
            "Epoch [54 / 80], Step [300 / 500], Loss: 0.2837\n",
            "Epoch [54 / 80], Step [400 / 500], Loss: 0.1515\n",
            "Epoch [54 / 80], Step [500 / 500], Loss: 0.2879\n",
            "Epoch [55 / 80], Step [100 / 500], Loss: 0.1593\n",
            "Epoch [55 / 80], Step [200 / 500], Loss: 0.1148\n",
            "Epoch [55 / 80], Step [300 / 500], Loss: 0.1734\n",
            "Epoch [55 / 80], Step [400 / 500], Loss: 0.1611\n",
            "Epoch [55 / 80], Step [500 / 500], Loss: 0.2149\n",
            "Epoch [56 / 80], Step [100 / 500], Loss: 0.1432\n",
            "Epoch [56 / 80], Step [200 / 500], Loss: 0.3259\n",
            "Epoch [56 / 80], Step [300 / 500], Loss: 0.2137\n",
            "Epoch [56 / 80], Step [400 / 500], Loss: 0.1839\n",
            "Epoch [56 / 80], Step [500 / 500], Loss: 0.3047\n",
            "Epoch [57 / 80], Step [100 / 500], Loss: 0.1255\n",
            "Epoch [57 / 80], Step [200 / 500], Loss: 0.1321\n",
            "Epoch [57 / 80], Step [300 / 500], Loss: 0.1774\n",
            "Epoch [57 / 80], Step [400 / 500], Loss: 0.1027\n",
            "Epoch [57 / 80], Step [500 / 500], Loss: 0.2283\n",
            "Epoch [58 / 80], Step [100 / 500], Loss: 0.1903\n",
            "Epoch [58 / 80], Step [200 / 500], Loss: 0.1544\n",
            "Epoch [58 / 80], Step [300 / 500], Loss: 0.1142\n",
            "Epoch [58 / 80], Step [400 / 500], Loss: 0.2676\n",
            "Epoch [58 / 80], Step [500 / 500], Loss: 0.2236\n",
            "Epoch [59 / 80], Step [100 / 500], Loss: 0.0703\n",
            "Epoch [59 / 80], Step [200 / 500], Loss: 0.1363\n",
            "Epoch [59 / 80], Step [300 / 500], Loss: 0.2352\n",
            "Epoch [59 / 80], Step [400 / 500], Loss: 0.1694\n",
            "Epoch [59 / 80], Step [500 / 500], Loss: 0.1585\n",
            "Epoch [60 / 80], Step [100 / 500], Loss: 0.2237\n",
            "Epoch [60 / 80], Step [200 / 500], Loss: 0.1760\n",
            "Epoch [60 / 80], Step [300 / 500], Loss: 0.1220\n",
            "Epoch [60 / 80], Step [400 / 500], Loss: 0.2511\n",
            "Epoch [60 / 80], Step [500 / 500], Loss: 0.2567\n",
            "Epoch [61 / 80], Step [100 / 500], Loss: 0.1772\n",
            "Epoch [61 / 80], Step [200 / 500], Loss: 0.1365\n",
            "Epoch [61 / 80], Step [300 / 500], Loss: 0.1197\n",
            "Epoch [61 / 80], Step [400 / 500], Loss: 0.1690\n",
            "Epoch [61 / 80], Step [500 / 500], Loss: 0.2373\n",
            "Epoch [62 / 80], Step [100 / 500], Loss: 0.2168\n",
            "Epoch [62 / 80], Step [200 / 500], Loss: 0.3079\n",
            "Epoch [62 / 80], Step [300 / 500], Loss: 0.0697\n",
            "Epoch [62 / 80], Step [400 / 500], Loss: 0.1118\n",
            "Epoch [62 / 80], Step [500 / 500], Loss: 0.1646\n",
            "Epoch [63 / 80], Step [100 / 500], Loss: 0.1852\n",
            "Epoch [63 / 80], Step [200 / 500], Loss: 0.2974\n",
            "Epoch [63 / 80], Step [300 / 500], Loss: 0.1429\n",
            "Epoch [63 / 80], Step [400 / 500], Loss: 0.1512\n",
            "Epoch [63 / 80], Step [500 / 500], Loss: 0.1618\n",
            "Epoch [64 / 80], Step [100 / 500], Loss: 0.1848\n",
            "Epoch [64 / 80], Step [200 / 500], Loss: 0.1103\n",
            "Epoch [64 / 80], Step [300 / 500], Loss: 0.1270\n",
            "Epoch [64 / 80], Step [400 / 500], Loss: 0.1416\n",
            "Epoch [64 / 80], Step [500 / 500], Loss: 0.2054\n",
            "Epoch [65 / 80], Step [100 / 500], Loss: 0.0844\n",
            "Epoch [65 / 80], Step [200 / 500], Loss: 0.1507\n",
            "Epoch [65 / 80], Step [300 / 500], Loss: 0.2336\n",
            "Epoch [65 / 80], Step [400 / 500], Loss: 0.1996\n",
            "Epoch [65 / 80], Step [500 / 500], Loss: 0.0966\n",
            "Epoch [66 / 80], Step [100 / 500], Loss: 0.0770\n",
            "Epoch [66 / 80], Step [200 / 500], Loss: 0.2221\n",
            "Epoch [66 / 80], Step [300 / 500], Loss: 0.2191\n",
            "Epoch [66 / 80], Step [400 / 500], Loss: 0.1277\n",
            "Epoch [66 / 80], Step [500 / 500], Loss: 0.0871\n",
            "Epoch [67 / 80], Step [100 / 500], Loss: 0.1916\n",
            "Epoch [67 / 80], Step [200 / 500], Loss: 0.1714\n",
            "Epoch [67 / 80], Step [300 / 500], Loss: 0.1005\n",
            "Epoch [67 / 80], Step [400 / 500], Loss: 0.0946\n",
            "Epoch [67 / 80], Step [500 / 500], Loss: 0.1559\n",
            "Epoch [68 / 80], Step [100 / 500], Loss: 0.0884\n",
            "Epoch [68 / 80], Step [200 / 500], Loss: 0.1161\n",
            "Epoch [68 / 80], Step [300 / 500], Loss: 0.1299\n",
            "Epoch [68 / 80], Step [400 / 500], Loss: 0.1725\n",
            "Epoch [68 / 80], Step [500 / 500], Loss: 0.1748\n",
            "Epoch [69 / 80], Step [100 / 500], Loss: 0.2526\n",
            "Epoch [69 / 80], Step [200 / 500], Loss: 0.1498\n",
            "Epoch [69 / 80], Step [300 / 500], Loss: 0.1753\n",
            "Epoch [69 / 80], Step [400 / 500], Loss: 0.1250\n",
            "Epoch [69 / 80], Step [500 / 500], Loss: 0.1515\n",
            "Epoch [70 / 80], Step [100 / 500], Loss: 0.2119\n",
            "Epoch [70 / 80], Step [200 / 500], Loss: 0.2023\n",
            "Epoch [70 / 80], Step [300 / 500], Loss: 0.2425\n",
            "Epoch [70 / 80], Step [400 / 500], Loss: 0.1278\n",
            "Epoch [70 / 80], Step [500 / 500], Loss: 0.1846\n",
            "Epoch [71 / 80], Step [100 / 500], Loss: 0.2227\n",
            "Epoch [71 / 80], Step [200 / 500], Loss: 0.1397\n",
            "Epoch [71 / 80], Step [300 / 500], Loss: 0.2255\n",
            "Epoch [71 / 80], Step [400 / 500], Loss: 0.1978\n",
            "Epoch [71 / 80], Step [500 / 500], Loss: 0.1706\n",
            "Epoch [72 / 80], Step [100 / 500], Loss: 0.1160\n",
            "Epoch [72 / 80], Step [200 / 500], Loss: 0.1456\n",
            "Epoch [72 / 80], Step [300 / 500], Loss: 0.1220\n",
            "Epoch [72 / 80], Step [400 / 500], Loss: 0.2399\n",
            "Epoch [72 / 80], Step [500 / 500], Loss: 0.1274\n",
            "Epoch [73 / 80], Step [100 / 500], Loss: 0.1365\n",
            "Epoch [73 / 80], Step [200 / 500], Loss: 0.1394\n",
            "Epoch [73 / 80], Step [300 / 500], Loss: 0.1118\n",
            "Epoch [73 / 80], Step [400 / 500], Loss: 0.1049\n",
            "Epoch [73 / 80], Step [500 / 500], Loss: 0.1087\n",
            "Epoch [74 / 80], Step [100 / 500], Loss: 0.1831\n",
            "Epoch [74 / 80], Step [200 / 500], Loss: 0.0736\n",
            "Epoch [74 / 80], Step [300 / 500], Loss: 0.1388\n",
            "Epoch [74 / 80], Step [400 / 500], Loss: 0.1516\n",
            "Epoch [74 / 80], Step [500 / 500], Loss: 0.0598\n",
            "Epoch [75 / 80], Step [100 / 500], Loss: 0.1303\n",
            "Epoch [75 / 80], Step [200 / 500], Loss: 0.0985\n",
            "Epoch [75 / 80], Step [300 / 500], Loss: 0.1806\n",
            "Epoch [75 / 80], Step [400 / 500], Loss: 0.1785\n",
            "Epoch [75 / 80], Step [500 / 500], Loss: 0.1965\n",
            "Epoch [76 / 80], Step [100 / 500], Loss: 0.1629\n",
            "Epoch [76 / 80], Step [200 / 500], Loss: 0.2170\n",
            "Epoch [76 / 80], Step [300 / 500], Loss: 0.1763\n",
            "Epoch [76 / 80], Step [400 / 500], Loss: 0.1584\n",
            "Epoch [76 / 80], Step [500 / 500], Loss: 0.1877\n",
            "Epoch [77 / 80], Step [100 / 500], Loss: 0.1791\n",
            "Epoch [77 / 80], Step [200 / 500], Loss: 0.1270\n",
            "Epoch [77 / 80], Step [300 / 500], Loss: 0.1357\n",
            "Epoch [77 / 80], Step [400 / 500], Loss: 0.1824\n",
            "Epoch [77 / 80], Step [500 / 500], Loss: 0.1851\n",
            "Epoch [78 / 80], Step [100 / 500], Loss: 0.1655\n",
            "Epoch [78 / 80], Step [200 / 500], Loss: 0.1232\n",
            "Epoch [78 / 80], Step [300 / 500], Loss: 0.1314\n",
            "Epoch [78 / 80], Step [400 / 500], Loss: 0.1183\n",
            "Epoch [78 / 80], Step [500 / 500], Loss: 0.1288\n",
            "Epoch [79 / 80], Step [100 / 500], Loss: 0.1194\n",
            "Epoch [79 / 80], Step [200 / 500], Loss: 0.1811\n",
            "Epoch [79 / 80], Step [300 / 500], Loss: 0.1021\n",
            "Epoch [79 / 80], Step [400 / 500], Loss: 0.1062\n",
            "Epoch [79 / 80], Step [500 / 500], Loss: 0.1144\n",
            "Epoch [80 / 80], Step [100 / 500], Loss: 0.1686\n",
            "Epoch [80 / 80], Step [200 / 500], Loss: 0.1968\n",
            "Epoch [80 / 80], Step [300 / 500], Loss: 0.1914\n",
            "Epoch [80 / 80], Step [400 / 500], Loss: 0.1134\n",
            "Epoch [80 / 80], Step [500 / 500], Loss: 0.2138\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Plh5dtVcjHhK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "29cd5ac1-a4f9-4988-b58f-521698b7e112"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  for images, labels in test_loader:\n",
        "    images = images.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = model(images)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    total += labels.size(0)\n",
        "    correct += (predicted == labels).sum().item()\n",
        "    \n",
        "  print('Accuracy of the model on the test images: {} %'\n",
        "        .format(100 * correct / total))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the model on the test images: 88.32 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8I1VsqmmTvb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Save the model checkpoint\n",
        "\n",
        "torch.save(model.state_dict(), 'resnet.ckpt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}